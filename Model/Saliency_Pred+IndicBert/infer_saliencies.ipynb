{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tH261GjA-10"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9Ju9SGeB6_t"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "base_path=\"/scratch/username/saliency_summaries/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vss2iL6eBh3Y"
   },
   "outputs": [],
   "source": [
    "class SaliencyClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 nhead=1,\n",
    "                 nlayers=1,\n",
    "                 use_cls=True,\n",
    "                 cls_bail_embed=None,\n",
    "                 d_model=768):\n",
    "\n",
    "        super(SaliencyClassifier, self).__init__()\n",
    "\n",
    "        self.saliency_classifier = nn.Linear(d_model, 2)\n",
    "\n",
    "        # self.bail_classifier = nn.Linear(d_model, 2)\n",
    "\n",
    "        ## Use [cls] token or pooling output for bail prediction\n",
    "        # self.use_cls = use_cls\n",
    "\n",
    "        # if use_cls:\n",
    "        #     self.cls_bail_embed = cls_bail_embed ## (1,1,d_model)\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoder(nn.TransformerEncoderLayer(\n",
    "                                                            d_model=d_model, \n",
    "                                                            nhead=nhead,\n",
    "                                                            batch_first=True), \n",
    "                                                      nlayers, \n",
    "                                                      norm=None)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        ## x: (batch_size, padded_length, 768)\n",
    "        batch_size = x.size()[0]\n",
    "        \n",
    "\n",
    "        x = self.encoder_layer(x)\n",
    "\n",
    "        # bail_logits = self.bail_classifier(bail_x)  ## (batch_size, 2) \n",
    "\n",
    "        saliency_logits = self.saliency_classifier(x) ## (batch_size, padded_length, 2) \n",
    "\n",
    "        return saliency_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvybQ1NkB5Rw"
   },
   "outputs": [],
   "source": [
    "model = SaliencyClassifier(d_model=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y628bnXVCRI4",
    "outputId": "44b5180a-2c40-435c-c585-0f15943acb7d"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('sc-all.pt', map_location=torch.device('cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('device: ' + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1gGpZX3C3PH",
    "outputId": "6dcac820-bbd8-4f7e-e3cb-799a74f00104"
   },
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09DOsHJTC91u"
   },
   "outputs": [],
   "source": [
    "def clean_text(row):\n",
    "    text = []\n",
    "    [text.extend(i.strip().split('ред')) for i in row]\n",
    "    text = [i.strip() for i in text]\n",
    "    text = list(filter(None, text))\n",
    "    return text\n",
    "\n",
    "def clean_dataset():\n",
    "    train = pd.read_csv(f'{base_path}train_split_alldistrict_bail.csv')\n",
    "    test = pd.read_csv(f'{base_path}test_split_alldistricts.csv')\n",
    "    val = pd.read_csv(f'{base_path}val_split_alldistrict.csv')\n",
    "    #train=train.head(500)\n",
    "    #test=test.head(500)\n",
    "    #val=val.head(500)\n",
    "\n",
    "    train['ranked-sentences'] = train['ranked-sentences'].apply(eval)\n",
    "    test['ranked-sentences'] = test['ranked-sentences'].apply(eval)\n",
    "    val['ranked-sentences'] = val['ranked-sentences'].apply(eval)\n",
    "\n",
    "    train['segments'] = train['segments'].apply(eval)\n",
    "    test['segments'] = test['segments'].apply(eval)\n",
    "    val['segments'] = val['segments'].apply(eval)\n",
    "\n",
    "    train['ranked-sentences'] = train['ranked-sentences'].apply(clean_text)\n",
    "    test['ranked-sentences'] = test['ranked-sentences'].apply(clean_text)\n",
    "    val['ranked-sentences'] = val['ranked-sentences'].apply(clean_text)\n",
    "\n",
    "    train['facts-and-arguments'] = train['segments'].apply(lambda x: clean_text(x['facts-and-arguments']))\n",
    "    test['facts-and-arguments'] = test['segments'].apply(lambda x: clean_text(x['facts-and-arguments']))\n",
    "    val['facts-and-arguments'] = val['segments'].apply(lambda x: clean_text(x['facts-and-arguments']))\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        # self.decisions = self.df.decision.map({'dismissed': 0, 'granted': 1})\n",
    "        self.ranked_sentences = self.df['ranked-sentences']\n",
    "\n",
    "        self.sentence_model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        lines = self.df.iloc[idx]['facts-and-arguments']\n",
    "        embeddings = self.sentence_model.encode(\n",
    "            lines\n",
    "        )\n",
    "\n",
    "        labels = [0] * len(lines)\n",
    "        indices = [lines.index(i) for i in self.ranked_sentences.iloc[idx]]\n",
    "        for i in indices[:len(labels)//2]:\n",
    "            labels[i] = 1\n",
    "\n",
    "\n",
    "        sample['embeddings'] = torch.from_numpy(embeddings)\n",
    "        # sample['bail'] = torch.Tensor([self.decisions.iloc[idx]])\n",
    "        # sample['salience_labels'] = torch.LongTensor(labels)\n",
    "\n",
    "        return sample \n",
    "\n",
    "def custom_collate(batch):\n",
    "\n",
    "    # bails, labels, embs = [], [], []\n",
    "    labels, embs = [], []\n",
    "    for item in batch:\n",
    "        # bails.append(item['bail'])\n",
    "        # labels.append(item['salience_labels'])\n",
    "        embs.append(item['embeddings'])\n",
    "\n",
    "    # bails = pad_sequence(bails, batch_first=True)\n",
    "    embs = pad_sequence(embs, batch_first=True)\n",
    "    # labels = pad_sequence(labels, padding_value=-100, batch_first=True)\n",
    "    # return embs, bails.long(), labels.long()\n",
    "    # return embs, labels.long()\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCG_g6YfYmU8"
   },
   "outputs": [],
   "source": [
    "train, val, test = clean_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5YOwRYpbfpT"
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train)\n",
    "val_dataset = Dataset(val)\n",
    "test_dataset = Dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXkLePm5YpYf"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=custom_collate\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=custom_collate\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=custom_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8x2OHFKcUlQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_predictions = []\n",
    "for idx, batch in tqdm(enumerate(train_dataloader),total=len(train_dataloader)):\n",
    "  with torch.no_grad():\n",
    "      batch = batch.to(device)\n",
    "      preds = model(batch).detach().cpu()\n",
    "      train_predictions.append(preds.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AWxpMyjdROn"
   },
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "for idx, batch in tqdm(enumerate(test_dataloader),total=len(test_dataloader)):\n",
    "  with torch.no_grad():\n",
    "      batch = batch.to(device)\n",
    "      preds = model(batch).detach().cpu()\n",
    "      test_predictions.append(preds.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0G6ZPf0Xf_Sv"
   },
   "outputs": [],
   "source": [
    "val_predictions = []\n",
    "for idx, batch in tqdm(enumerate(val_dataloader),total=len(val_dataloader)):\n",
    "  with torch.no_grad():\n",
    "      batch = batch.to(device)\n",
    "      preds = model(batch).detach().cpu()\n",
    "      val_predictions.append(preds.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5L45DZW9gB0n"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/scratch/username/o1/train_preds.pkl', 'wb') as f:\n",
    "  pickle.dump(train_predictions, f)\n",
    "\n",
    "with open('/scratch/username/o1/test_preds.pkl', 'wb') as f:\n",
    "  pickle.dump(test_predictions, f)\n",
    "\n",
    "with open('/scratch/username/o1/val_preds.pkl', 'wb') as f:\n",
    "  pickle.dump(val_predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2pt8qhHaVfP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "saliency-inference.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
