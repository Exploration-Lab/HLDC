{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.tokenize import indic_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir(\"path to folder/legal_train_test_data/combined_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "def put_in_corpus(file):\n",
    "  global corpus\n",
    "  print(f\"Current file:{file}\")\n",
    "  if file != \"train_split_alldistrict_bail.json\":\n",
    "      return\n",
    "  with open(f\"path to folder/legal_train_test_data/combined_data/{file}\") as f:\n",
    "      data = json.load(f)\n",
    "  print(\"DATA LOADED\")\n",
    "  df = pd.DataFrame(data)\n",
    "  for i, row in tqdm(df.iterrows(),total=len(df)):    \n",
    "      src_sents=[]\n",
    "      paras = df.loc[i][\"segments\"]['facts-and-arguments']\n",
    "      for para in paras:\n",
    "        sent = para.split('ред')\n",
    "        sent = [i for i in sent if len(i)!=0 and i!=' ']\n",
    "        src_sents.extend(sent)\n",
    "      src_sents = list(filter(None, src_sents))\n",
    "      src_sents1=[]\n",
    "      for sent in src_sents:\n",
    "        try:\n",
    "          sent = ''.join([i for i in sent if not i.isdigit()])\n",
    "        except:\n",
    "          print(sent)\n",
    "        sent = indic_tokenize.trivial_tokenize(sent)\n",
    "        src_sents1.append(sent)\n",
    "      src_sents =src_sents1\n",
    "      corpus.extend(src_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files[:]:\n",
    "  put_in_corpus(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "open_file = open(\"path to folder/textRank/model_all_districts/words_final.pkl\", \"wb\")\n",
    "pickle.dump(corpus, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"path to folder/textRank/model_all_districts/words_final.pkl\",'rb')\n",
    "corpus = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "model = FastText(sentences=corpus, size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"path to folder/textRank/model_all_districts/model_final.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "model = FastText.load('path to folder/textRank/model_all_districts/model_final.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(file):\n",
    "    print(f\"Current file:{file}\")\n",
    "    with open(f\"path to folder/legal_train_test_data/combined_data/{file}\") as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(data)\n",
    "    ranked_sentences = []\n",
    "    for i, row in tqdm(df.iterrows(),total=len(df)):\n",
    "        src_sents = df.loc[i][\"segments\"]['facts-and-arguments']\n",
    "        src_sents = [i.split('ред') for i in src_sents]\n",
    "        src_sents = [i for i in src_sents if len(i)!=0 and i!=' ']\n",
    "        src_sents = [i for subl in src_sents for i in subl]\n",
    "        src_sents = list(filter(None, src_sents))\n",
    "        src_sents1=[]\n",
    "        for sent in src_sents:\n",
    "          try:\n",
    "            sent = ''.join([i for i in sent if not i.isdigit()])\n",
    "          except:\n",
    "            print(sent)\n",
    "          src_sents1.append(sent)\n",
    "        src_sents =src_sents1\n",
    "        sentences = src_sents\n",
    "        sentence_vectors = []\n",
    "        sentences = [elt for elt in sentences if elt != ' ']\n",
    "        n = len(sentences)\n",
    "        for sent in sentences:\n",
    "          if len(i) != 0:\n",
    "            tokens = indic_tokenize.trivial_tokenize(sent)\n",
    "            try:\n",
    "              v = sum([model.wv[w] for w in tokens])/(len(tokens) + 0.001)\n",
    "            except KeyError:\n",
    "              print('sent', sent)\n",
    "              print('tokens', tokens)\n",
    "          else:\n",
    "              v = np.zeros((100,))\n",
    "          sentence_vectors.append(v)\n",
    "\n",
    "        # similarity matrix\n",
    "        sim_mat = np.zeros([n, n])\n",
    "        for i in range(n):\n",
    "          for j in range(n):\n",
    "              if i != j:\n",
    "                  sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "        nx_graph = nx.from_numpy_array(sim_mat)\n",
    "        scores = nx.pagerank_numpy(nx_graph)\n",
    "        ranked_sentences.append(sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True))\n",
    "\n",
    "    print(len(df), len(ranked_sentences))\n",
    "    df = df.head(len(ranked_sentences))\n",
    "    df['ranked-sentences'] = [i[:10] for i in ranked_sentences]\n",
    "    df['ranked-sentences'].map(len)\n",
    "    \n",
    "    file = file.replace(\".json\",\".csv\")\n",
    "    df.to_csv(f\"path to folder/textRank_summaries/{file}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_districts_split = [\"train_split_alldistrict_bail.json\", \"val_split_alldistrict.json\", \"test_split_alldistricts.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files[:]:\n",
    "    if file == \"validation_split_10_districts.json\":\n",
    "        process(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"validation_split_10_districts.json\"\n",
    "with open(f\"path to folder/legal_train_test_data/combined_data/{file}\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = pd.read_csv(\"path to folder/textRank_summaries/validation_split_10_districts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries['ranked-sentences'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "ast.literal_eval(summaries['segments'][0])['facts-and-arguments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ast.literal_eval(summaries['segments'][0])['facts-and-arguments']:\n",
    "    j = i.split(\"ред\")\n",
    "    for k in j:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
